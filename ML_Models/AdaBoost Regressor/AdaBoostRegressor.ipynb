{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AdaBoostRegressor Model Theory**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "AdaBoostRegressor (Adaptive Boosting Regressor) is a machine learning model that combines multiple weak regressors (typically decision trees) into a strong ensemble. It works by iteratively training regressors while adjusting the weights of training samples based on their prediction errors. Unlike gradient boosting, AdaBoost uses a weighted training set where misclassified samples get higher weights in subsequent iterations.\n",
    "\n",
    "The model function for AdaBoostRegressor is a weighted sum of weak regressors:\n",
    "$$ f(x) = \\sum_{t=1}^{T} w_t h_t(x) $$\n",
    "Where:\n",
    "- $f(x)$ is the final prediction\n",
    "- $T$ is the number of weak regressors\n",
    "- $w_t$ is the weight assigned to regressor $t$\n",
    "- $h_t(x)$ is the prediction of regressor $t$ for input $x$\n",
    "\n",
    "## Model Training\n",
    "### Forward Pass\n",
    "During training, AdaBoostRegressor iteratively builds weak regressors (typically decision trees) where each regressor tries to correct the errors of the ensemble by focusing on the harder-to-predict samples through sample weighting.\n",
    "\n",
    "### Loss Functions\n",
    "AdaBoostRegressor supports three types of loss functions:\n",
    "1. **Linear**: Uses linear loss for error calculation\n",
    "2. **Square**: Uses squared error loss\n",
    "3. **Exponential**: Uses exponential loss function\n",
    "\n",
    "### AdaBoost.R2 Algorithm\n",
    "AdaBoostRegressor implements the AdaBoost.R2 algorithm which follows these steps:\n",
    "1. Initialize sample weights equally: $D_1(i) = 1/N$\n",
    "2. For each iteration $t=1,\\dots,T$:\n",
    "   - Train weak regressor $h_t$ using weighted samples\n",
    "   - Calculate individual error for each sample\n",
    "   - Compute weighted error: $\\epsilon_t = \\sum_{i=1}^{N} D_t(i)E_t(i)$\n",
    "   - Calculate regressor weight: $w_t = \\log((1-\\epsilon_t)/\\epsilon_t)$\n",
    "   - Update sample weights for next iteration\n",
    "\n",
    "## Training Process\n",
    "The training process involves these main steps:\n",
    "1. **Initialize**: Start with equal weights for all training samples\n",
    "2. **Iterate**:\n",
    "   - Train a weak regressor on weighted training data\n",
    "   - Calculate prediction errors and regressor weight\n",
    "   - Update sample weights (increase weights of poorly predicted samples)\n",
    "   - Add weighted regressor to ensemble\n",
    "3. **Combine**: Final prediction is weighted median of all regressors\n",
    "\n",
    "### Key Parameters\n",
    "- `n_estimators`: Controls the number of weak regressors (default=50)\n",
    "- `learning_rate`: Shrinks the contribution of each regressor (default=1.0)\n",
    "- `loss`: Type of loss function to use ('linear', 'square', 'exponential')\n",
    "- `estimator`: Base regressor to use (default=DecisionTreeRegressor(max_depth=3))\n",
    "\n",
    "### Differences from Gradient Boosting\n",
    "1. **Weight Updates**: AdaBoost updates sample weights, while gradient boosting fits to residuals\n",
    "2. **Base Estimators**: AdaBoost typically uses shallow trees, while gradient boosting can use deeper trees\n",
    "3. **Error Correction**: AdaBoost focuses on hard examples through weight updates, while gradient boosting directly optimizes the loss function\n",
    "4. **Prediction**: AdaBoost uses weighted median for final prediction, while gradient boosting uses sum of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean Squared Error (MSE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}_i} - y_{\\text{pred}_i})^2\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Mean Squared Error (MSE)** is a widely used metric for evaluating the accuracy of regression models.\n",
    "- It measures the average squared difference between the predicted values ($y_{\\text{pred}}$) and the actual target values ($y_{\\text{true}}$).\n",
    "- The squared differences are averaged across all data points in the dataset.\n",
    "\n",
    "**Interpretation:**\n",
    "- A lower MSE indicates a better fit of the model to the data, as it means the model's predictions are closer to the actual values.\n",
    "- MSE is sensitive to outliers because the squared differences magnify the impact of large errors.\n",
    "- **Limitations:**\n",
    "  - MSE can be hard to interpret because it is in squared units of the target variable.\n",
    "  - It disproportionately penalizes larger errors due to the squaring process.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Root Mean Squared Error (RMSE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Root Mean Squared Error (RMSE)** is a variant of MSE that provides the square root of the average squared difference between predicted and actual values.\n",
    "- It is often preferred because it is in the same unit as the target variable, making it more interpretable.\n",
    "\n",
    "**Interpretation:**\n",
    "- Like MSE, a lower RMSE indicates a better fit of the model to the data.\n",
    "- RMSE is also sensitive to outliers due to the square root operation.\n",
    "- **Advantages over MSE:**\n",
    "  - RMSE provides a more intuitive interpretation since it is in the same scale as the target variable.\n",
    "  - It can be more directly compared to the values of the actual data.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. R-squared ($R^2$)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **R-squared ($R^2$)**, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable ($y_{\\text{true}}$) that is predictable from the independent variable(s) ($y_{\\text{pred}}$) in a regression model.\n",
    "- It ranges from 0 to 1, where 0 indicates that the model does not explain any variance, and 1 indicates a perfect fit.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher $R^2$ value suggests that the model explains a larger proportion of the variance in the target variable.\n",
    "- However, $R^2$ does not provide information about the goodness of individual predictions or whether the model is overfitting or underfitting.\n",
    "- **Limitations:**\n",
    "  - $R^2$ can be misleading in cases of overfitting, especially with polynomial regression models. Even if $R^2$ is high, the model may not generalize well to unseen data.\n",
    "  - It doesnâ€™t penalize for adding irrelevant predictors, so adjusted $R^2$ is often preferred for models with multiple predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Adjusted R-squared\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\frac{n-1}{n-p-1}\n",
    "$$\n",
    "where \\(n\\) is the number of data points and \\(p\\) is the number of predictors.\n",
    "\n",
    "**Description:**\n",
    "- **Adjusted R-squared** adjusts the R-squared value to account for the number of predictors in the model, helping to prevent overfitting when adding more terms to the model.\n",
    "- Unlike $R^2$, it can decrease if the additional predictors do not improve the model significantly.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher adjusted $R^2$ suggests that the model is not just overfitting, but has genuine explanatory power with the number of predictors taken into account.\n",
    "- It is especially useful when comparing models with different numbers of predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Mean Absolute Error (MAE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{true}_i} - y_{\\text{pred}_i}|\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Mean Absolute Error (MAE)** measures the average of the absolute errors between the predicted and actual values.\n",
    "- Unlike MSE and RMSE, MAE is not sensitive to outliers because it does not square the errors.\n",
    "\n",
    "**Interpretation:**\n",
    "- MAE provides a straightforward understanding of the average error magnitude.\n",
    "- A lower MAE suggests better model accuracy, but it may not highlight the impact of large errors as much as MSE or RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn template [scikit-learn: AdaBoostRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html)\n",
    "\n",
    "### class sklearn.ensemble.AdaBoostRegressor(estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
    "\n",
    "| **Parameter**        | **Description**                                                                                          | **Default**  |\n",
    "|---------------------|----------------------------------------------------------------------------------------------------------|--------------|\n",
    "| `estimator`         | Base estimator from which boosted ensemble is built. If None, uses DecisionTreeRegressor(max_depth=3)     | `None`       |\n",
    "| `n_estimators`      | Maximum number of estimators at which boosting is terminated. Values must be in range [1, inf)            | `50`         |\n",
    "| `learning_rate`     | Weight applied to each regressor. Higher rate increases each regressor's contribution. Range (0.0, inf)    | `1.0`        |\n",
    "| `loss`              | Loss function to use when updating weights after each boosting iteration                                   | `'linear'`   |\n",
    "| `random_state`      | Controls random seed for estimator boosting iterations and weight bootstrapping                           | `None`       |\n",
    "\n",
    "---\n",
    "\n",
    "| **Attribute**           | **Description**                                                                                        |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------------|\n",
    "| `estimator_`           | The base estimator from which the ensemble is grown                                                    |\n",
    "| `estimators_`          | The collection of fitted sub-estimators                                                                |\n",
    "| `estimator_weights_`   | Weights for each estimator in the boosted ensemble                                                     |\n",
    "| `estimator_errors_`    | Regression error for each estimator in the boosted ensemble                                            |\n",
    "| `feature_importances_` | The impurity-based feature importances                                                                 |\n",
    "| `n_features_in_`       | Number of features seen during fit                                                                     |\n",
    "| `feature_names_in_`    | Names of features seen during fit (if X has feature names)                                             |\n",
    "\n",
    "---\n",
    "\n",
    "| **Method**             | **Description**                                                                                        |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------------|\n",
    "| `fit(X, y)`           | Build a boosted regressor from the training set (X, y)                                                 |\n",
    "| `predict(X)`          | Predict regression value for X using weighted median prediction                                         |\n",
    "| `score(X, y)`         | Return the coefficient of determination RÂ²                                                             |\n",
    "| `staged_predict(X)`   | Return staged predictions after each boosting iteration                                                |\n",
    "| `staged_score(X, y)`  | Return staged scores after each boosting iteration                                                     |\n",
    "| `get_params()`        | Get parameters of this estimator                                                                       |\n",
    "| `set_params(**params)`| Set parameters of this estimator                                                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XXXXXXXX regression - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
