{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gradient Boosting Regressor Model Theory**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "GradientBoostingRegressor is a machine learning model that uses the gradient boosting algorithm for regression tasks. It builds an ensemble of decision trees in a sequential manner, where each tree corrects the errors of the previous one. The algorithm minimizes a loss function (typically mean squared error for regression) by fitting each new tree to the residuals of the previous predictions. \n",
    "\n",
    "The model function for GradientBoostingRegressor is the sum of the outputs of all decision trees:\n",
    "\n",
    "$$ f(x) = \\sum_{t=1}^{T} \\alpha_t h_t(x) $$\n",
    "\n",
    "Where:\n",
    "- $f(x)$ is the predicted output.\n",
    "- $T$ is the number of trees.\n",
    "- $\\alpha_t$ is the weight of the $t$-th tree.\n",
    "- $h_t(x)$ is the prediction of the $t$-th tree for input $x$.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "During training, the forward pass in GradientBoostingRegressor involves iteratively building decision trees, each of which focuses on predicting the residuals (errors) of the previous trees. This helps the model correct errors made by previous trees and refine the predictions.\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "The cost function for GradientBoostingRegressor is the loss function used to evaluate how well the model's predictions match the actual outputs. In the case of regression, this is typically the mean squared error (MSE):\n",
    "\n",
    "$$ J(\\alpha) = \\frac{1}{m} \\sum_{i=1}^{m}(f(x^{(i)}) - y^{(i)})^2 $$\n",
    "\n",
    "Where:\n",
    "- $J(\\alpha)$ is the cost function (MSE).\n",
    "- $m$ is the number of training examples.\n",
    "- $f(x^{(i)})$ is the predicted output for the $i$-th example.\n",
    "- $y^{(i)}$ is the actual output for the $i$-th example.\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "GradientBoostingRegressor uses gradient descent to minimize the cost function by fitting new trees to the residuals of the previous prediction. The gradient of the cost function is calculated, and each tree is fit to the negative gradient of the loss function. This process is repeated for each tree.\n",
    "\n",
    "The update for each tree is:\n",
    "\n",
    "$$ h_t(x) = -\\frac{\\partial J}{\\partial f(x)} $$\n",
    "\n",
    "Where:\n",
    "- $h_t(x)$ is the tree added at stage $t$.\n",
    "- $\\frac{\\partial J}{\\partial f(x)}$ is the gradient of the cost function with respect to the prediction at each stage.\n",
    "\n",
    "## Training Process\n",
    "\n",
    "The training process in GradientBoostingRegressor involves the following steps:\n",
    "1. **Initial prediction**: Start with an initial prediction, which is typically the mean of the target variable.\n",
    "2. **Compute residuals**: Calculate the residuals, which are the differences between the actual target values and the current model predictions.\n",
    "3. **Fit decision trees**: Fit a decision tree to the residuals. This tree learns to predict the errors of the current model.\n",
    "4. **Update prediction**: Add the predictions of the new tree to the current prediction.\n",
    "5. **Repeat**: Repeat the above steps until the desired number of trees is reached or the model stops improving.\n",
    "\n",
    "GradientBoostingRegressor uses a learning rate ($\\alpha$) to control the contribution of each new tree. Smaller learning rates lead to more trees being used, while larger learning rates speed up the training but may increase the risk of overfitting.\n",
    "\n",
    "By iteratively improving the model through decision trees, GradientBoostingRegressor can learn complex patterns in the data and achieve high accuracy in regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean Squared Error (MSE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}_i} - y_{\\text{pred}_i})^2\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Mean Squared Error (MSE)** is a widely used metric for evaluating the accuracy of regression models.\n",
    "- It measures the average squared difference between the predicted values ($y_{\\text{pred}}$) and the actual target values ($y_{\\text{true}}$).\n",
    "- The squared differences are averaged across all data points in the dataset.\n",
    "\n",
    "**Interpretation:**\n",
    "- A lower MSE indicates a better fit of the model to the data, as it means the model's predictions are closer to the actual values.\n",
    "- MSE is sensitive to outliers because the squared differences magnify the impact of large errors.\n",
    "- **Limitations:**\n",
    "  - MSE can be hard to interpret because it is in squared units of the target variable.\n",
    "  - It disproportionately penalizes larger errors due to the squaring process.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Root Mean Squared Error (RMSE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Root Mean Squared Error (RMSE)** is a variant of MSE that provides the square root of the average squared difference between predicted and actual values.\n",
    "- It is often preferred because it is in the same unit as the target variable, making it more interpretable.\n",
    "\n",
    "**Interpretation:**\n",
    "- Like MSE, a lower RMSE indicates a better fit of the model to the data.\n",
    "- RMSE is also sensitive to outliers due to the square root operation.\n",
    "- **Advantages over MSE:**\n",
    "  - RMSE provides a more intuitive interpretation since it is in the same scale as the target variable.\n",
    "  - It can be more directly compared to the values of the actual data.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. R-squared ($R^2$)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **R-squared ($R^2$)**, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable ($y_{\\text{true}}$) that is predictable from the independent variable(s) ($y_{\\text{pred}}$) in a regression model.\n",
    "- It ranges from 0 to 1, where 0 indicates that the model does not explain any variance, and 1 indicates a perfect fit.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher $R^2$ value suggests that the model explains a larger proportion of the variance in the target variable.\n",
    "- However, $R^2$ does not provide information about the goodness of individual predictions or whether the model is overfitting or underfitting.\n",
    "- **Limitations:**\n",
    "  - $R^2$ can be misleading in cases of overfitting, especially with polynomial regression models. Even if $R^2$ is high, the model may not generalize well to unseen data.\n",
    "  - It doesnâ€™t penalize for adding irrelevant predictors, so adjusted $R^2$ is often preferred for models with multiple predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Adjusted R-squared\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\frac{n-1}{n-p-1}\n",
    "$$\n",
    "where \\(n\\) is the number of data points and \\(p\\) is the number of predictors.\n",
    "\n",
    "**Description:**\n",
    "- **Adjusted R-squared** adjusts the R-squared value to account for the number of predictors in the model, helping to prevent overfitting when adding more terms to the model.\n",
    "- Unlike $R^2$, it can decrease if the additional predictors do not improve the model significantly.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher adjusted $R^2$ suggests that the model is not just overfitting, but has genuine explanatory power with the number of predictors taken into account.\n",
    "- It is especially useful when comparing models with different numbers of predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Mean Absolute Error (MAE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{true}_i} - y_{\\text{pred}_i}|\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Mean Absolute Error (MAE)** measures the average of the absolute errors between the predicted and actual values.\n",
    "- Unlike MSE and RMSE, MAE is not sensitive to outliers because it does not square the errors.\n",
    "\n",
    "**Interpretation:**\n",
    "- MAE provides a straightforward understanding of the average error magnitude.\n",
    "- A lower MAE suggests better model accuracy, but it may not highlight the impact of large errors as much as MSE or RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn template [scikit-learn: GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)\n",
    "\n",
    "### class sklearn.ensemble.GradientBoostingRegressor(*, loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, max_depth=3, min_impurity_decrease=0.0, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=1e-4, ccp_alpha=0.0)\n",
    "\n",
    "| **Parameter**          | **Description**                                                                                                                                                                | **Default**     |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|\n",
    "| `loss`                 | Loss function to be minimized. Can be 'ls' (least squares) or other loss types.                                                                                               | `'ls'`          |\n",
    "| `learning_rate`        | Shrinks the contribution of each tree by this factor. Higher values make the model more sensitive to the data.                                                                  | `0.1`           |\n",
    "| `n_estimators`         | Number of boosting stages to be run.                                                                                                                                             | `100`           |\n",
    "| `subsample`            | Proportion of samples used for fitting each tree. Values between 0.0 and 1.0.                                                                                                | `1.0`           |\n",
    "| `criterion`            | The function to measure the quality of a split. 'friedman_mse' is the most commonly used.                                                                                       | `'friedman_mse'`|\n",
    "| `min_samples_split`    | Minimum number of samples required to split an internal node.                                                                                                                  | `2`             |\n",
    "| `min_samples_leaf`     | Minimum number of samples required to be at a leaf node.                                                                                                                       | `1`             |\n",
    "| `max_depth`            | Maximum depth of the individual trees.                                                                                                                                           | `3`             |\n",
    "| `min_impurity_decrease`| Minimum impurity decrease required for a split to occur.                                                                                                                        | `0.0`           |\n",
    "| `random_state`         | Controls the randomness of the estimator. Used for reproducibility.                                                                                                            | `None`          |\n",
    "| `max_features`         | The number of features to consider when looking for the best split. Can be integer, float, or specific strings.                                                               | `None`          |\n",
    "| `alpha`                | The alpha-quantile of the huber loss function and the quantile loss function. Used if `loss='huber'` or `loss='quantile'`.                                                    | `0.9`           |\n",
    "| `verbose`              | Controls the verbosity of the output.                                                                                                                                           | `0`             |\n",
    "| `max_leaf_nodes`       | Maximum number of leaf nodes in the trees.                                                                                                                                       | `None`          |\n",
    "| `warm_start`           | If True, reuse the solution of the previous call to fit and add more estimators.                                                                                                | `False`         |\n",
    "| `validation_fraction`  | Proportion of training data set aside as validation set for early stopping.                                                                                                     | `0.1`           |\n",
    "| `n_iter_no_change`     | Number of iterations with no improvement in validation score before stopping training.                                                                                         | `None`          |\n",
    "| `tol`                  | Tolerance for the early stopping. Training will stop if the validation score is not improving by at least `tol` for `n_iter_no_change` iterations.                             | `1e-4`          |\n",
    "| `ccp_alpha`            | Complexity parameter used for Minimal Cost-Complexity Pruning. A subtree with cost complexity smaller than `ccp_alpha` is selected.                                            | `0.0`           |\n",
    "\n",
    "-\n",
    "\n",
    "| **Attribute**          | **Description**                                                                                                                                                                |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `n_estimators_`        | Number of boosting stages (i.e., trees) used in the model.                                                                                                                     |\n",
    "| `feature_importances_` | Impurity-based feature importances.                                                                                                                                             |\n",
    "| `oob_improvement_`     | Improvement in loss on the out-of-bag samples relative to the previous iteration.                                                                                             |\n",
    "| `train_score_`         | The training score at each iteration.                                                                                                                                           |\n",
    "| `estimators_`          | The collection of fitted sub-estimators (decision trees).                                                                                                                      |\n",
    "| `n_features_in_`       | The number of features seen during fitting.                                                                                                                                     |\n",
    "| `feature_names_in_`    | Names of features seen during fitting (if applicable).                                                                                                                         |\n",
    "\n",
    "-\n",
    "\n",
    "| **Method**             | **Description**                                                                                                                                                                |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `fit(X, y)`            | Fit the gradient boosting model to the input data `X` and target values `y`.                                                                                                  |\n",
    "| `predict(X)`           | Predict regression target for the input data `X`.                                                                                                                              |\n",
    "| `score(X, y)`          | Return the coefficient of determination (RÂ² score) for the prediction.                                                                                                        |\n",
    "| `get_params()`         | Gets the parameters of the GradientBoostingRegressor model.                                                                                                                     |\n",
    "| `set_params(**params)` | Sets the parameters of the GradientBoostingRegressor model.                                                                                                                     |\n",
    "| `apply(X)`             | Apply trees in the ensemble to `X`, returning the leaf indices for each tree.                                                                                                  |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XXXXXXXX regression - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
