{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LgithGBMRegressor Model Theory**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "LightGBMRegressor is a gradient boosting framework that uses decision trees for regression tasks, but it is optimized for speed and efficiency. It is designed to handle large datasets with high dimensionality and supports categorical features natively. LightGBM uses a technique called gradient-based one-side sampling (GOSS) and exclusive feature bundling (EFB) to speed up the training process while maintaining high accuracy.\n",
    "\n",
    "The model function for LightGBMRegressor is similar to other gradient boosting algorithms, where the final prediction is a weighted sum of the predictions from all decision trees:\n",
    "\n",
    "$$ f(x) = \\sum_{t=1}^{T} \\alpha_t h_t(x) $$\n",
    "\n",
    "Where:\n",
    "- $f(x)$ is the predicted output.\n",
    "- $T$ is the number of trees.\n",
    "- $\\alpha_t$ is the weight of the $t$-th tree.\n",
    "- $h_t(x)$ is the prediction of the $t$-th tree for input $x$.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "In LightGBM, the forward pass builds decision trees iteratively. Each tree is trained to predict the residuals of the previous predictions, and the training process is accelerated using optimizations like GOSS and EFB. The goal is to correct the errors made by the previous trees in the ensemble.\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "LightGBMRegressor typically uses the mean squared error (MSE) as the loss function for regression tasks. The objective function is:\n",
    "\n",
    "$$ J(\\alpha) = \\frac{1}{m} \\sum_{i=1}^{m}(f(x^{(i)}) - y^{(i)})^2 $$\n",
    "\n",
    "Where:\n",
    "- $J(\\alpha)$ is the cost function (MSE).\n",
    "- $m$ is the number of training examples.\n",
    "- $f(x^{(i)})$ is the predicted output for the $i$-th example.\n",
    "- $y^{(i)}$ is the actual output for the $i$-th example.\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "LightGBMRegressor applies gradient boosting using the gradient of the loss function to update the decision trees. It employs gradient-based one-side sampling (GOSS), which helps speed up the training process by focusing on the samples with large gradients, and exclusive feature bundling (EFB), which groups features that rarely appear together to reduce the dimensionality.\n",
    "\n",
    "The update for each tree is similar to other gradient boosting algorithms:\n",
    "\n",
    "$$ h_t(x) = -\\frac{\\partial J}{\\partial f(x)} $$\n",
    "\n",
    "Where:\n",
    "- $h_t(x)$ is the tree added at stage $t$.\n",
    "- $\\frac{\\partial J}{\\partial f(x)}$ is the gradient of the cost function with respect to the prediction at each stage.\n",
    "\n",
    "## Training Process\n",
    "\n",
    "The training process in LightGBMRegressor involves the following steps:\n",
    "1. **Initial prediction**: Start with an initial prediction, typically the mean of the target variable.\n",
    "2. **Compute residuals**: Compute the residuals (errors) by subtracting the current predictions from the actual target values.\n",
    "3. **Fit decision trees**: Fit decision trees to the residuals, using optimizations like GOSS and EFB to speed up the training.\n",
    "4. **Update prediction**: Add the output of the new tree to the current prediction.\n",
    "5. **Repeat**: Repeat the above steps for a predefined number of trees or until further improvement is minimal.\n",
    "\n",
    "LightGBMRegressor also supports early stopping, where training can be halted if the model's performance stops improving on a validation set. It also has hyperparameters for controlling the learning rate, number of trees, tree depth, and regularization techniques to prevent overfitting.\n",
    "\n",
    "By iteratively adding decision trees and using advanced optimizations, LightGBMRegressor can efficiently handle large datasets and complex patterns, while maintaining high accuracy in regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean Squared Error (MSE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}_i} - y_{\\text{pred}_i})^2\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Mean Squared Error (MSE)** is a widely used metric for evaluating the accuracy of regression models.\n",
    "- It measures the average squared difference between the predicted values ($y_{\\text{pred}}$) and the actual target values ($y_{\\text{true}}$).\n",
    "- The squared differences are averaged across all data points in the dataset.\n",
    "\n",
    "**Interpretation:**\n",
    "- A lower MSE indicates a better fit of the model to the data, as it means the model's predictions are closer to the actual values.\n",
    "- MSE is sensitive to outliers because the squared differences magnify the impact of large errors.\n",
    "- **Limitations:**\n",
    "  - MSE can be hard to interpret because it is in squared units of the target variable.\n",
    "  - It disproportionately penalizes larger errors due to the squaring process.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Root Mean Squared Error (RMSE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Root Mean Squared Error (RMSE)** is a variant of MSE that provides the square root of the average squared difference between predicted and actual values.\n",
    "- It is often preferred because it is in the same unit as the target variable, making it more interpretable.\n",
    "\n",
    "**Interpretation:**\n",
    "- Like MSE, a lower RMSE indicates a better fit of the model to the data.\n",
    "- RMSE is also sensitive to outliers due to the square root operation.\n",
    "- **Advantages over MSE:**\n",
    "  - RMSE provides a more intuitive interpretation since it is in the same scale as the target variable.\n",
    "  - It can be more directly compared to the values of the actual data.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. R-squared ($R^2$)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **R-squared ($R^2$)**, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable ($y_{\\text{true}}$) that is predictable from the independent variable(s) ($y_{\\text{pred}}$) in a regression model.\n",
    "- It ranges from 0 to 1, where 0 indicates that the model does not explain any variance, and 1 indicates a perfect fit.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher $R^2$ value suggests that the model explains a larger proportion of the variance in the target variable.\n",
    "- However, $R^2$ does not provide information about the goodness of individual predictions or whether the model is overfitting or underfitting.\n",
    "- **Limitations:**\n",
    "  - $R^2$ can be misleading in cases of overfitting, especially with polynomial regression models. Even if $R^2$ is high, the model may not generalize well to unseen data.\n",
    "  - It doesnâ€™t penalize for adding irrelevant predictors, so adjusted $R^2$ is often preferred for models with multiple predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Adjusted R-squared\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\frac{n-1}{n-p-1}\n",
    "$$\n",
    "where \\(n\\) is the number of data points and \\(p\\) is the number of predictors.\n",
    "\n",
    "**Description:**\n",
    "- **Adjusted R-squared** adjusts the R-squared value to account for the number of predictors in the model, helping to prevent overfitting when adding more terms to the model.\n",
    "- Unlike $R^2$, it can decrease if the additional predictors do not improve the model significantly.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher adjusted $R^2$ suggests that the model is not just overfitting, but has genuine explanatory power with the number of predictors taken into account.\n",
    "- It is especially useful when comparing models with different numbers of predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Mean Absolute Error (MAE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{true}_i} - y_{\\text{pred}_i}|\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Mean Absolute Error (MAE)** measures the average of the absolute errors between the predicted and actual values.\n",
    "- Unlike MSE and RMSE, MAE is not sensitive to outliers because it does not square the errors.\n",
    "\n",
    "**Interpretation:**\n",
    "- MAE provides a straightforward understanding of the average error magnitude.\n",
    "- A lower MAE suggests better model accuracy, but it may not highlight the impact of large errors as much as MSE or RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM template [LightGBM: LGBMRegressor](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html)\n",
    "\n",
    "### class lightgbm.LGBMRegressor(*, boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=1e-3, min_child_samples=20, subsample=1.0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, silent=False, importance_type='split')\n",
    "\n",
    "| **Parameter**          | **Description**                                                                                                                                                                | **Default**     |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|\n",
    "| `boosting_type`        | Type of boosting model. Options: 'gbdt', 'dart', 'goss', 'rf'.                                                                                                                 | `'gbdt'`        |\n",
    "| `num_leaves`           | Maximum number of leaves in one tree.                                                                                                                                            | `31`            |\n",
    "| `max_depth`            | Maximum depth of a tree. -1 means no limit.                                                                                                                                     | `-1`            |\n",
    "| `learning_rate`        | Step size shrinking to prevent overfitting.                                                                                                                                     | `0.1`           |\n",
    "| `n_estimators`         | Number of boosting iterations (trees).                                                                                                                                         | `100`           |\n",
    "| `subsample_for_bin`    | Number of data points used for constructing histograms in the algorithm.                                                                                                       | `200000`        |\n",
    "| `objective`            | Objective function. For regression, use 'regression' or other options.                                                                                                         | `None`          |\n",
    "| `class_weight`         | Class weights for classification tasks.                                                                                                                                         | `None`          |\n",
    "| `min_split_gain`       | Minimum gain to make a further partition.                                                                                                                                       | `0.0`           |\n",
    "| `min_child_weight`     | Minimum sum of weights (or samples) in a child.                                                                                                                                 | `1e-3`          |\n",
    "| `min_child_samples`    | Minimum number of samples required to be in a child.                                                                                                                           | `20`            |\n",
    "| `subsample`            | Fraction of samples used for fitting each tree.                                                                                                                                 | `1.0`           |\n",
    "| `colsample_bytree`     | Fraction of features to consider for each tree.                                                                                                                                | `1.0`           |\n",
    "| `reg_alpha`            | L1 regularization term on weights.                                                                                                                                              | `0.0`           |\n",
    "| `reg_lambda`           | L2 regularization term on weights.                                                                                                                                              | `0.0`           |\n",
    "| `random_state`         | Seed for reproducibility.                                                                                                                                                       | `None`          |\n",
    "| `n_jobs`               | Number of parallel threads used for training.                                                                                                                                   | `-1`            |\n",
    "| `silent`               | Whether to suppress output messages.                                                                                                                                             | `False`         |\n",
    "| `importance_type`      | The method to calculate feature importance. Options: 'split', 'gain'.                                                                                                          | `'split'`       |\n",
    "\n",
    "-\n",
    "\n",
    "| **Attribute**          | **Description**                                                                                                                                                                |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `best_iteration_`      | Number of iterations chosen based on early stopping.                                                                                                                            |\n",
    "| `feature_importances_` | Feature importances based on the splitting criterion (if `importance_type='split'`) or the total gain (if `importance_type='gain'`).                                           |\n",
    "| `n_features_in_`       | Number of features seen during fitting.                                                                                                                                         |\n",
    "| `booster_`             | The trained booster (LightGBM model object).                                                                                                                                   |\n",
    "\n",
    "-\n",
    "\n",
    "| **Method**             | **Description**                                                                                                                                                                |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `fit(X, y)`            | Fit the model to the training data `X` and target values `y`.                                                                                                                 |\n",
    "| `predict(X)`           | Predict regression target for the input data `X`.                                                                                                                              |\n",
    "| `score(X, y)`          | Return the RÂ² score for the prediction.                                                                                                                                         |\n",
    "| `get_params()`         | Get the parameters of the LGBMRegressor model.                                                                                                                                   |\n",
    "| `set_params(**params)` | Set the parameters of the LGBMRegressor model.                                                                                                                                   |\n",
    "| `feature_importances_` | Returns the importance of features based on training.                                                                                                                           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XXXXXXXX regression - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
