{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SVR Model Theory**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "Support Vector Regressor (SVR) is a regression model based on the Support Vector Machine (SVM) algorithm. It is designed to find a function that approximates the target variable while maintaining a margin of tolerance for the error. The goal of SVR is to find a function that lies within a predefined margin (epsilon) from the actual data points, with the least amount of error outside this margin. The SVR aims to achieve both a low training error and high generalization ability by controlling the complexity of the model.\n",
    "\n",
    "The model function for SVR is:\n",
    "\n",
    "$$ f(x) = w^T x + b $$\n",
    "\n",
    "Where:\n",
    "- $f(x)$ is the predicted output.\n",
    "- $w$ is the weight vector (the model coefficients).\n",
    "- $x$ is the input feature vector.\n",
    "- $b$ is the bias term.\n",
    "\n",
    "SVR differs from regular SVM because it allows for errors within a margin (controlled by epsilon), and the goal is to minimize the complexity of the model while respecting the margin constraints.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "In the forward pass of SVR, the model makes predictions using the learned weights and bias. The predictions are based on the learned decision function:\n",
    "\n",
    "$$ f(x) = w^T x + b $$\n",
    "\n",
    "Where the decision function is used to approximate the target variable, while ensuring that the margin defined by epsilon is respected.\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "The cost function in SVR is designed to penalize predictions that fall outside the margin. The epsilon-insensitive loss function is used, which penalizes predictions that are further than epsilon from the actual target value. The cost function is:\n",
    "\n",
    "$$ J(w,b,\\epsilon) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{m} \\max(0, |f(x^{(i)}) - y^{(i)}| - \\epsilon) $$\n",
    "\n",
    "Where:\n",
    "- $J(w,b,\\epsilon)$ is the cost function.\n",
    "- $w$ is the weight vector.\n",
    "- $b$ is the bias.\n",
    "- $m$ is the number of training examples.\n",
    "- $x^{(i)}$ and $y^{(i)}$ are the input and actual output for the $i$-th example.\n",
    "- $\\epsilon$ is the margin of tolerance for errors.\n",
    "- $C$ is the regularization parameter that controls the trade-off between model complexity and the margin of tolerance.\n",
    "\n",
    "The first term, $\\frac{1}{2} \\|w\\|^2$, represents the regularization term that aims to minimize the model complexity (i.e., finding a simpler model with smaller weights). The second term penalizes data points that fall outside the epsilon margin.\n",
    "\n",
    "### Gradient Computation\n",
    "\n",
    "The gradient of the cost function with respect to the weights and bias is computed to update the model parameters during training. The update equations are based on the principle of minimizing the error, while also ensuring that the margin constraints are satisfied.\n",
    "\n",
    "## Training Process\n",
    "\n",
    "The training process of an SVR involves finding the optimal weights ($w$) and bias ($b$) that minimize the cost function while satisfying the epsilon margin. This is typically done through convex optimization techniques such as Quadratic Programming (QP) or using more efficient algorithms like Sequential Minimal Optimization (SMO).\n",
    "\n",
    "The steps are as follows:\n",
    "1. **Initial prediction**: Start with an initial guess for the weights and bias, often set to zero.\n",
    "2. **Compute loss**: Calculate the loss function based on the current weights and bias, including both the margin constraint and the regularization term.\n",
    "3. **Update parameters**: Update the weights and bias using optimization techniques like gradient descent or SMO to minimize the cost function.\n",
    "4. **Repeat**: Repeat the process until the model parameters converge or a stopping criterion is met.\n",
    "\n",
    "SVR also supports tuning of hyperparameters such as the regularization parameter $C$, the margin parameter $\\epsilon$, and the kernel used in the transformation of the input space (e.g., linear, polynomial, or RBF kernels).\n",
    "\n",
    "By adjusting the regularization and margin parameters, SVR can be optimized to make accurate predictions while avoiding overfitting, providing a robust model for regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean Squared Error (MSE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}_i} - y_{\\text{pred}_i})^2\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Mean Squared Error (MSE)** is a widely used metric for evaluating the accuracy of regression models.\n",
    "- It measures the average squared difference between the predicted values ($y_{\\text{pred}}$) and the actual target values ($y_{\\text{true}}$).\n",
    "- The squared differences are averaged across all data points in the dataset.\n",
    "\n",
    "**Interpretation:**\n",
    "- A lower MSE indicates a better fit of the model to the data, as it means the model's predictions are closer to the actual values.\n",
    "- MSE is sensitive to outliers because the squared differences magnify the impact of large errors.\n",
    "- **Limitations:**\n",
    "  - MSE can be hard to interpret because it is in squared units of the target variable.\n",
    "  - It disproportionately penalizes larger errors due to the squaring process.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Root Mean Squared Error (RMSE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Root Mean Squared Error (RMSE)** is a variant of MSE that provides the square root of the average squared difference between predicted and actual values.\n",
    "- It is often preferred because it is in the same unit as the target variable, making it more interpretable.\n",
    "\n",
    "**Interpretation:**\n",
    "- Like MSE, a lower RMSE indicates a better fit of the model to the data.\n",
    "- RMSE is also sensitive to outliers due to the square root operation.\n",
    "- **Advantages over MSE:**\n",
    "  - RMSE provides a more intuitive interpretation since it is in the same scale as the target variable.\n",
    "  - It can be more directly compared to the values of the actual data.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. R-squared ($R^2$)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **R-squared ($R^2$)**, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable ($y_{\\text{true}}$) that is predictable from the independent variable(s) ($y_{\\text{pred}}$) in a regression model.\n",
    "- It ranges from 0 to 1, where 0 indicates that the model does not explain any variance, and 1 indicates a perfect fit.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher $R^2$ value suggests that the model explains a larger proportion of the variance in the target variable.\n",
    "- However, $R^2$ does not provide information about the goodness of individual predictions or whether the model is overfitting or underfitting.\n",
    "- **Limitations:**\n",
    "  - $R^2$ can be misleading in cases of overfitting, especially with polynomial regression models. Even if $R^2$ is high, the model may not generalize well to unseen data.\n",
    "  - It doesnâ€™t penalize for adding irrelevant predictors, so adjusted $R^2$ is often preferred for models with multiple predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Adjusted R-squared\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\frac{n-1}{n-p-1}\n",
    "$$\n",
    "where \\(n\\) is the number of data points and \\(p\\) is the number of predictors.\n",
    "\n",
    "**Description:**\n",
    "- **Adjusted R-squared** adjusts the R-squared value to account for the number of predictors in the model, helping to prevent overfitting when adding more terms to the model.\n",
    "- Unlike $R^2$, it can decrease if the additional predictors do not improve the model significantly.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher adjusted $R^2$ suggests that the model is not just overfitting, but has genuine explanatory power with the number of predictors taken into account.\n",
    "- It is especially useful when comparing models with different numbers of predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Mean Absolute Error (MAE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{true}_i} - y_{\\text{pred}_i}|\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Mean Absolute Error (MAE)** measures the average of the absolute errors between the predicted and actual values.\n",
    "- Unlike MSE and RMSE, MAE is not sensitive to outliers because it does not square the errors.\n",
    "\n",
    "**Interpretation:**\n",
    "- MAE provides a straightforward understanding of the average error magnitude.\n",
    "- A lower MAE suggests better model accuracy, but it may not highlight the impact of large errors as much as MSE or RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn template [scikit-learn: SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "\n",
    "### class sklearn.svm.SVR(*, C=1.0, epsilon=0.1, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, tol=1e-3, cache_size=200, verbose=False, max_iter=-1)\n",
    "\n",
    "| **Parameter**        | **Description**                                                                                          | **Default**  |\n",
    "|----------------------|----------------------------------------------------------------------------------------------------------|--------------|\n",
    "| `C`                  | Regularization parameter. The strength of regularization (higher means more regularization).              | `1.0`        |\n",
    "| `epsilon`            | Epsilon parameter for the epsilon-SVR model. Specifies the epsilon-tube within which no penalty is associated. | `0.1`        |\n",
    "| `kernel`             | Specifies the kernel type ('linear', 'poly', 'rbf', etc.).                                                | `'rbf'`      |\n",
    "| `degree`             | Degree of the polynomial kernel function ('poly'). Ignored by other kernels.                             | `3`          |\n",
    "| `gamma`              | Kernel coefficient for â€˜rbfâ€™, â€˜polyâ€™, and â€˜sigmoidâ€™.                                                     | `'scale'`    |\n",
    "| `coef0`              | Independent term in kernel function.                                                                     | `0.0`        |\n",
    "| `shrinking`          | Whether to use the shrinking heuristic.                                                                   | `True`       |\n",
    "| `tol`                | Tolerance for stopping criteria.                                                                          | `1e-3`       |\n",
    "| `cache_size`         | Size of the kernel cache in MB.                                                                          | `200`        |\n",
    "| `verbose`            | Whether to print progress messages during fitting.                                                       | `False`      |\n",
    "| `max_iter`           | The maximum number of iterations. -1 means no limit.                                                     | `-1`         |\n",
    "\n",
    "---\n",
    "\n",
    "| **Attribute**        | **Description**                                                                                           |\n",
    "|----------------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| `coef_`              | Weights assigned to the features (only for 'linear' kernel).                                                |\n",
    "| `dual_coef_`         | Coefficients of the support vector in the decision function.                                                |\n",
    "| `intercept_`         | Constants in decision function.                                                                            |\n",
    "| `n_features_in_`     | Number of features seen during fit.                                                                        |\n",
    "| `feature_names_in_`  | Names of features seen during fit (only if `X` contains feature names).                                    |\n",
    "| `n_iter_`            | Number of iterations run by the optimization routine.                                                      |\n",
    "| `n_support_`         | Number of support vectors for each class.                                                                  |\n",
    "| `support_`           | Indices of support vectors.                                                                                |\n",
    "| `support_vectors_`   | Support vectors.                                                                                           |\n",
    "\n",
    "---\n",
    "\n",
    "| **Method**           | **Description**                                                                                           |\n",
    "|----------------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| `fit(X, y)`          | Fit the model to the training data `X` and target values `y`.                                              |\n",
    "| `predict(X)`         | Predict regression target for the input data `X`.                                                         |\n",
    "| `score(X, y)`        | Return the coefficient of determination (RÂ² score).                                                       |\n",
    "| `get_params()`       | Get the parameters of the SVR model.                                                                       |\n",
    "| `set_params(**params)`| Set the parameters of the SVR model.                                                                      |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XXXXXXXX regression - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
